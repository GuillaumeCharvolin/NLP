{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP2 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy, developper information to prompt for T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from itertools import product\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_important_phrases(value):\n",
    "    \"\"\"\n",
    "    Extracts key phrases from the text, including verbs, adjectives, nouns, and named entities.\n",
    "    \"\"\"\n",
    "    doc = nlp(value)\n",
    "    key_phrases = []\n",
    "\n",
    "    # Extract named entities\n",
    "    for ent in doc.ents:\n",
    "        key_phrases.append(ent.text)\n",
    "\n",
    "    # Extract important verbs, adjectives, and noun phrases\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"VERB\", \"ADJ\", \"NOUN\", \"PROPN\"}:  # Include verbs, adjectives, nouns, and proper nouns\n",
    "            key_phrases.append(token.text)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    key_phrases = list(dict.fromkeys(key_phrases))\n",
    "    \n",
    "    return \", \".join(key_phrases) if key_phrases else value\n",
    "\n",
    "def generate_flexible_context(data):\n",
    "    context = \"Player context: \"\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        key_doc = nlp(key.replace(\"_\", \" \"))\n",
    "        key_phrase = \" \".join([token.text for token in key_doc])\n",
    "        \n",
    "        context += f\"{key_phrase.capitalize()} is {value}. \"\n",
    "    \n",
    "    return context.strip()\n",
    "\n",
    "def transform_to_prompts(data):\n",
    "    prompts = []\n",
    "    player_variations = []\n",
    "    npc_context_parts = []\n",
    "\n",
    "    for field, details in data.items():\n",
    "        field_type = details.get(\"type\")\n",
    "        value = details.get(\"value\")\n",
    "        \n",
    "        if field_type == \"text\":\n",
    "            extracted_info = extract_important_phrases(value)\n",
    "            npc_context_parts.append(f\"{field}: {extracted_info}\")\n",
    "        \n",
    "        elif field_type == \"choice\":\n",
    "            options = [opt.strip() for opt in value.split(\",\")]\n",
    "            player_variations.append([(field, option) for option in options])\n",
    "\n",
    "    npc_context = \" | \".join(npc_context_parts)\n",
    "\n",
    "    player_state_combinations = product(*player_variations)\n",
    "    for player_state in player_state_combinations:\n",
    "        player_state_str = \" | \".join([f\"{field} (Player): {state}\" for field, state in player_state])\n",
    "        \n",
    "        prompt = (\n",
    "            f\"Generate dialogue considering the following:\\n\"\n",
    "            f\"NPC Context: {npc_context}\\n\"\n",
    "            f\"Player State: {player_state_str}.\"\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def generate_dialogue(prompts, num_responses=2):\n",
    "    dialogue_options = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=50,\n",
    "            num_return_sequences=num_responses,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        dialogues = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        dialogue_options.extend(dialogues)\n",
    "\n",
    "    return dialogue_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Charger le modèle et le tokenizer de BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Retourne l'embedding du texte donné en utilisant BERT.\"\"\"\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = bert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]  # Prendre le vecteur de [CLS]\n",
    "    return embeddings\n",
    "\n",
    "def score_similarity(context, dialogue):\n",
    "    \"\"\"Calcule la similarité cosinus entre le contexte et le dialogue.\"\"\"\n",
    "    context_embedding = embed_text(context)\n",
    "    dialogue_embedding = embed_text(dialogue)\n",
    "    similarity = F.cosine_similarity(context_embedding, dialogue_embedding)\n",
    "    return similarity.item()\n",
    "\n",
    "def select_final_dialogue(context, filtered_dialogues):\n",
    "    \"\"\"Choisit le dialogue ayant la meilleure correspondance avec le contexte.\"\"\"\n",
    "    best_score = -1\n",
    "    best_dialogue = None\n",
    "    \n",
    "    score_debug = {}\n",
    "\n",
    "    for dialogue in filtered_dialogues:\n",
    "        score = score_similarity(context, dialogue)\n",
    "        score_debug[dialogue] = score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_dialogue = dialogue\n",
    "    \n",
    "    return best_dialogue, score_debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Debug Info Functions ---\n",
    "\n",
    "def generate_debug_info(input_data, prompts):\n",
    "    \"\"\"Return debug information for dialogue generation step.\"\"\"\n",
    "    debug_info = {\"Received Input Data\": input_data, 'prompts': prompts}\n",
    "    return debug_info\n",
    "\n",
    "def select_debug_info(input_data):\n",
    "    \"\"\"Return debug information for dialogue selection/filtering step.\"\"\"\n",
    "    return input_data\n",
    "\n",
    "def choices_debug_info(input_data, score_debug):\n",
    "    \"\"\"Return debug information for final dialogue confirmation step.\"\"\"\n",
    "    debug_response = {\"Message received\": input_data, \"Score Debug\": score_debug}\n",
    "    return debug_response\n",
    "\n",
    "# --- Core Processing Functions ---\n",
    "\n",
    "def generate_dialogue_variations(input_data):\n",
    "    \"\"\"Generate multiple dialogue options based on game context and player status.\"\"\"\n",
    "\n",
    "    prompts = transform_to_prompts(input_data)\n",
    "    dialogue = generate_dialogue(prompts)\n",
    "\n",
    "    return prompts, dialogue\n",
    "\n",
    "def filter_dialogue_variations(selected_dialogues):\n",
    "    \"\"\"Filter out unwanted dialogue options based on developer input.\"\"\"\n",
    "    return\n",
    "\n",
    "def select_best_dialogue(player_state, filtered_dialogues):\n",
    "    \"\"\"Select the best dialogue option matching the player’s current state.\"\"\"\n",
    "    \n",
    "    context = generate_flexible_context(player_state)\n",
    "    selected_final_dialogue, score_debug = select_final_dialogue(context, filtered_dialogues)\n",
    "\n",
    "    return selected_final_dialogue, score_debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Function to free up a port if necessary\n",
    "def free_port(port=5000):\n",
    "    for conn in psutil.net_connections(kind='inet'):\n",
    "        if conn.laddr.port == port:\n",
    "            pid = conn.pid\n",
    "            if pid:\n",
    "                os.kill(pid, 9)\n",
    "\n",
    "# Ensure port 5000 is free before starting Flask\n",
    "free_port()\n",
    "\n",
    "# --- API Endpoints ---\n",
    "\n",
    "@app.route('/generate_dialogue_options', methods=['POST'])\n",
    "def generate_dialogue_options():\n",
    "    input_data = request.get_json()\n",
    "    debug = input_data.pop('debug', False)\n",
    "\n",
    "    response_payload = {}\n",
    "    prompts, dialogue_options = generate_dialogue_variations(input_data)\n",
    "\n",
    "    response_payload['dialogue_options'] = dialogue_options\n",
    "\n",
    "    if debug:\n",
    "        response_payload[\"debug_info\"] = generate_debug_info(input_data, prompts)\n",
    "\n",
    "    return jsonify(response_payload)\n",
    "\n",
    "filtered_dialogues_storage = {}\n",
    "\n",
    "@app.route('/filter_dialogue_options', methods=['POST'])\n",
    "def filter_dialogue_options():\n",
    "    input_data = request.get_json()\n",
    "    debug = input_data.get(\"debug\", False)\n",
    "\n",
    "    # Identifier l'utilisateur ou générer un ID si nécessaire\n",
    "    user_id = input_data.get(\"user_id\", \"default_user\")  # Il est mieux de passer un `user_id` unique par utilisateur\n",
    "    \n",
    "    response_payload = {}\n",
    "    \n",
    "    # Stocker les dialogues filtrés dans le dictionnaire\n",
    "    filtered_dialogues_storage[user_id] = input_data.get(\"selected_options\", [])\n",
    "\n",
    "    if debug:\n",
    "        response_payload[\"debug_info\"] = select_debug_info(input_data)\n",
    "\n",
    "    return jsonify(response_payload)\n",
    "\n",
    "@app.route('/confirm_final_dialogue', methods=['POST'])\n",
    "def confirm_final_dialogue():\n",
    "    input_data = request.get_json()\n",
    "    debug = input_data.pop(\"debug\", False)\n",
    "\n",
    "    user_id = input_data.get(\"user_id\", \"default_user\")\n",
    "\n",
    "    filtered_dialogue = filtered_dialogues_storage.get(user_id)\n",
    "    \n",
    "    response_payload = {}\n",
    "    response_payload['final_selected_dialogue'], score_debug = select_best_dialogue(input_data, filtered_dialogue)\n",
    "\n",
    "    if debug:\n",
    "        response_payload[\"debug_info\"] = choices_debug_info(input_data, score_debug)\n",
    "\n",
    "    return jsonify(response_payload)\n",
    "\n",
    "# --- Run Flask App ---\n",
    "\n",
    "def run_app():\n",
    "    app.run(port=5000)\n",
    "\n",
    "# Start the Flask server in a separate thread\n",
    "flask_thread = threading.Thread(target=run_app)\n",
    "flask_thread.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
